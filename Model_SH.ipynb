{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf51c2d8-0a6f-4112-ac4c-e297043341c7",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da2dd5-7769-4a57-a067-abe071bbc6a6",
   "metadata": {},
   "source": [
    "The following code for an MLP was very lightly modified from https://machinelearningmastery.com/pytorch-tutorial-develop-deep-learning-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37f4a85-a168-4941-9463-0b9b4cff8b85",
   "metadata": {},
   "source": [
    "- We Started with 2 epochs, 2 hidden layers, learning rate 0.015, 2 epochs. Got an F1 score of 0.901. \n",
    "- Upped epochs to 10, F1 score was still around 0.9. \n",
    "- Added a third hidden layer with RELU activation function. Got an F1 score of 0.97 with 2 epochs. The rest of the testing was done with the additional hidden layer as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c982484-caeb-4c2f-8934-6bddaf966da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the data\n",
    "data = pd.read_csv('..\\Project\\eventmatrixlabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffdcfc3-2185-4754-82ce-02210389bcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch mlp for binary classification\n",
    "from numpy import vstack\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import Sigmoid\n",
    "from torch.nn import Module\n",
    "from torch.optim import SGD\n",
    "from torch.nn import BCELoss\n",
    "from torch.nn.init import kaiming_uniform_\n",
    "from torch.nn.init import xavier_uniform_\n",
    " \n",
    "# dataset definition\n",
    "class logDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self):\n",
    "        # load the csv file as a dataframe\n",
    "        # store the inputs and outputs\n",
    "        self.X = data.drop(['Label', 'BlockId'], axis=1).values[:,:]\n",
    "        self.y = y = data[['Label']].values[:,-1]\n",
    "        # ensure input data is floats\n",
    "        self.X = self.X.astype('float32')\n",
    "        # label encode target and ensure the values are floats\n",
    "        self.y = LabelEncoder().fit_transform(self.y)\n",
    "        self.y = self.y.astype('float32')\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    " \n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    " \n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    " \n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.15, n_val = 0.05):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        val_size = round(n_val * len(self.X))\n",
    "        train_size = len(self.X) - test_size - val_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, val_size, test_size])\n",
    " \n",
    " # model definition\n",
    "class MLP(Module):\n",
    "    # define model elements\n",
    "    def __init__(self, n_inputs):\n",
    "        super(MLP, self).__init__()\n",
    "        # input to first hidden layer\n",
    "        self.hidden1 = Linear(n_inputs, 30)\n",
    "        kaiming_uniform_(self.hidden1.weight, nonlinearity='relu')\n",
    "        self.act1 = ReLU()\n",
    "        # second hidden layer\n",
    "        self.hidden2 = Linear(30, 20)\n",
    "        kaiming_uniform_(self.hidden2.weight, nonlinearity='relu')\n",
    "        self.act2 = ReLU()\n",
    "        # third hidden layer\n",
    "        self.hidden3 = Linear(20, 10)\n",
    "        kaiming_uniform_(self.hidden3.weight, nonlinearity='relu')\n",
    "        self.act3 = ReLU()\n",
    "        # fourth hidden layer and output\n",
    "        self.hidden4 = Linear(10, 1)\n",
    "        xavier_uniform_(self.hidden3.weight)\n",
    "        self.act4 = Sigmoid()\n",
    " \n",
    "    # forward propagate input\n",
    "    def forward(self, X):\n",
    "        # input to first hidden layer\n",
    "        X = self.hidden1(X)\n",
    "        X = self.act1(X)\n",
    "         # second hidden layer\n",
    "        X = self.hidden2(X)\n",
    "        X = self.act2(X)\n",
    "        \n",
    "        X = self.hidden3(X)\n",
    "        X = self.act3(X)\n",
    "        \n",
    "        # third hidden layer and output\n",
    "        X = self.hidden4(X)\n",
    "        X = self.act4(X)\n",
    "        return X\n",
    " \n",
    " # prepare the dataset\n",
    "def prepare_data():\n",
    "    # load the dataset\n",
    "    dataset = logDataset()\n",
    "    # calculate split\n",
    "    train, val, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    train_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    val_dl = DataLoader(train, batch_size=1024, shuffle=True)\n",
    "    test_dl = DataLoader(test, batch_size=1024, shuffle=False)\n",
    "    return train_dl, val_dl, test_dl\n",
    " \n",
    "# train the model\n",
    "def train_model(train_dl, model, num_epochs = 2, lr = 0.01):\n",
    "    NUM_EPOCHS = 2\n",
    "    # define the optimization\n",
    "    criterion = BCELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # enumerate mini batches\n",
    "        for i, (inputs, targets) in enumerate(train_dl):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "        print (f'Epoch: {epoch+1:03d}/{num_epochs:03d} | Loss: {loss:.4f}')\n",
    " \n",
    " # evaluate the model\n",
    "def evaluate_model(test_dl, model):\n",
    "    predictions, actuals = list(), list()\n",
    "    for i, (inputs, targets) in enumerate(test_dl):\n",
    "        # evaluate the model on the test set\n",
    "        yhat = model(inputs)\n",
    "        # retrieve numpy array\n",
    "        yhat = yhat.detach().numpy()\n",
    "        actual = targets.numpy()\n",
    "        actual = actual.reshape((len(actual), 1))\n",
    "        # round to class values\n",
    "        yhat = yhat.round()\n",
    "        # store\n",
    "        predictions.append(yhat)\n",
    "        actuals.append(actual)\n",
    "    predictions, actuals = vstack(predictions), vstack(actuals)\n",
    "    # calculate f1 score \n",
    "    acc = sklearn.metrics.f1_score(actuals, predictions)\n",
    "    return acc\n",
    " \n",
    "# make a class prediction for one row of data\n",
    "def predict(row, model):\n",
    "    # convert row to data\n",
    "    row = Tensor([row])\n",
    "    # make prediction\n",
    "    yhat = model(row)\n",
    "    # retrieve numpy array\n",
    "    yhat = yhat.detach().numpy()\n",
    "    return yhat\n",
    " \n",
    "# prepare the data\n",
    "train_dl, val_dl, test_dl = prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a17ab42-e323-49ec-a605-e14acd978d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/002 | Loss: 0.0520\n",
      "Epoch: 002/002 | Loss: 0.0047\n",
      "F1 Score: 0.997\n"
     ]
    }
   ],
   "source": [
    "# define the network\n",
    "model = MLP(48)\n",
    "# train the model\n",
    "train_model(train_dl, model)\n",
    "#evaluate the model\n",
    "acc = evaluate_model(val_dl, model)\n",
    "print('F1 Score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76fa4709-fa3f-4070-9bbd-6f1661fcff1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/010 | Loss: 0.0706\n",
      "Epoch: 002/010 | Loss: 0.0112\n",
      "Epoch: 003/010 | Loss: 0.0016\n",
      "Epoch: 004/010 | Loss: 0.0010\n",
      "Epoch: 005/010 | Loss: 0.0020\n",
      "Epoch: 006/010 | Loss: 0.0004\n",
      "Epoch: 007/010 | Loss: 0.0012\n",
      "Epoch: 008/010 | Loss: 0.0238\n",
      "Epoch: 009/010 | Loss: 0.0001\n",
      "Epoch: 010/010 | Loss: 0.0002\n",
      "F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "model = MLP(48)\n",
    "# train the model\n",
    "train_model(train_dl, model, num_epochs = 10)\n",
    "#evaluate the model\n",
    "acc = evaluate_model(val_dl, model)\n",
    "print('F1 Score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30e73485-f926-460c-8c89-c2a269fd398a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Loss: 0.0790\n",
      "Epoch: 002/020 | Loss: 0.0571\n",
      "Epoch: 003/020 | Loss: 0.0074\n",
      "Epoch: 004/020 | Loss: 0.0053\n",
      "Epoch: 005/020 | Loss: 0.0033\n",
      "Epoch: 006/020 | Loss: 0.0021\n",
      "Epoch: 007/020 | Loss: 0.0011\n",
      "Epoch: 008/020 | Loss: 0.0005\n",
      "Epoch: 009/020 | Loss: 0.0192\n",
      "Epoch: 010/020 | Loss: 0.0011\n",
      "Epoch: 011/020 | Loss: 0.0002\n",
      "Epoch: 012/020 | Loss: 0.0008\n",
      "Epoch: 013/020 | Loss: 0.0009\n",
      "Epoch: 014/020 | Loss: 0.0020\n",
      "Epoch: 015/020 | Loss: 0.0004\n",
      "Epoch: 016/020 | Loss: 0.0000\n",
      "Epoch: 017/020 | Loss: 0.0004\n",
      "Epoch: 018/020 | Loss: 0.0001\n",
      "Epoch: 019/020 | Loss: 0.0003\n",
      "Epoch: 020/020 | Loss: 0.0005\n",
      "F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "model = MLP(48)\n",
    "# train the model\n",
    "train_model(train_dl, model, num_epochs = 20)\n",
    "#evaluate the model\n",
    "acc = evaluate_model(val_dl, model)\n",
    "print('F1 Score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9dd4514f-9892-414e-a3aa-d6a39331b209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/020 | Loss: 0.0991\n",
      "Epoch: 002/020 | Loss: 0.0708\n",
      "Epoch: 003/020 | Loss: 0.1562\n",
      "Epoch: 004/020 | Loss: 0.0766\n",
      "Epoch: 005/020 | Loss: 0.0702\n",
      "Epoch: 006/020 | Loss: 0.0761\n",
      "Epoch: 007/020 | Loss: 0.0247\n",
      "Epoch: 008/020 | Loss: 0.0481\n",
      "Epoch: 009/020 | Loss: 0.0384\n",
      "Epoch: 010/020 | Loss: 0.0363\n",
      "Epoch: 011/020 | Loss: 0.0296\n",
      "Epoch: 012/020 | Loss: 0.0438\n",
      "Epoch: 013/020 | Loss: 0.0334\n",
      "Epoch: 014/020 | Loss: 0.0562\n",
      "Epoch: 015/020 | Loss: 0.0091\n",
      "Epoch: 016/020 | Loss: 0.0330\n",
      "Epoch: 017/020 | Loss: 0.0152\n",
      "Epoch: 018/020 | Loss: 0.0082\n",
      "Epoch: 019/020 | Loss: 0.0053\n",
      "Epoch: 020/020 | Loss: 0.0032\n",
      "F1 Score: 0.999\n"
     ]
    }
   ],
   "source": [
    "model = MLP(48)\n",
    "# train the model\n",
    "train_model(train_dl, model, lr = 0.001, num_epochs = 20)\n",
    "#evaluate the model\n",
    "acc = evaluate_model(val_dl, model)\n",
    "print('F1 Score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09721042-616a-409d-bddc-d6b0e45a0dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/015 | Loss: 0.0527\n",
      "Epoch: 002/015 | Loss: 0.0176\n",
      "Epoch: 003/015 | Loss: 0.0068\n",
      "Epoch: 004/015 | Loss: 0.0026\n",
      "Epoch: 005/015 | Loss: 0.0036\n",
      "Epoch: 006/015 | Loss: 0.0014\n",
      "Epoch: 007/015 | Loss: 0.0107\n",
      "Epoch: 008/015 | Loss: 0.0108\n",
      "Epoch: 009/015 | Loss: 0.0011\n",
      "Epoch: 010/015 | Loss: 0.0009\n",
      "Epoch: 011/015 | Loss: 0.0012\n",
      "Epoch: 012/015 | Loss: 0.0003\n",
      "Epoch: 013/015 | Loss: 0.0004\n",
      "Epoch: 014/015 | Loss: 0.0006\n",
      "Epoch: 015/015 | Loss: 0.0003\n",
      "F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "model = MLP(48)\n",
    "# train the model\n",
    "train_model(train_dl, model, num_epochs = 15)\n",
    "#evaluate the model\n",
    "acc = evaluate_model(val_dl, model)\n",
    "print('F1 Score: %.3f' % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb157c25-42f1-451c-8f5a-3463e9db2101",
   "metadata": {},
   "source": [
    "Since the final model with 15 epochs and a learning rate of 0.01 performed the best, we will use it as our final neural network and see how the test data performs on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6896b9ec-6236-40cc-9cf3-408a8b69b6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "#evaluate the model on the test data. \n",
    "acc = evaluate_model(train_dl, model)\n",
    "print('F1 Score: %.3f' % acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
